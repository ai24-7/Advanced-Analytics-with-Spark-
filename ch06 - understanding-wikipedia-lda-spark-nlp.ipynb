{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2bba92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a42debf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.config(\"spark.driver.memory\", \"8g\").\\\n",
    "                            config(\"spark.driver.maxResultSize\", \"0\").\\\n",
    "                            config(\"spark.kryoserializer.buffer.max\", \"2000M\").\\\n",
    "                            config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:4.0.2\").\\\n",
    "                            appName('chapter_6').\\\n",
    "                            getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd758d9",
   "metadata": {},
   "source": [
    "### Getting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e159bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! wikiextractor wikidump.xml\n",
    "\n",
    "# ! mv text wikidump\n",
    "! tree wikidump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21853cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -n 5 wikidump/AA/wiki_00"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4de3d89",
   "metadata": {},
   "source": [
    "### Spark NLP\n",
    "\n",
    "If you intend to use the PySpark shell, start the shell with the following command:  \n",
    "`pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12406b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sparknlp\n",
    "\n",
    "spark = sparknlp.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21028beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.base import DocumentAssembler, Finisher\n",
    "from sparknlp.annotator import (Lemmatizer, Stemmer,\n",
    "                                Tokenizer, Normalizer,\n",
    "                                StopWordsCleaner)\n",
    "from sparknlp.pretrained import PretrainedPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0157236",
   "metadata": {},
   "source": [
    "### Parsing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fd4d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source = 'wikidump/*/*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1da792f",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = spark.sparkContext.wholeTextFiles(data_source).toDF()\n",
    "raw_data.show(1, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954914d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as fun\n",
    "df = raw_data.withColumn('content', fun.explode(fun.split(fun.col(\"_2\"),\n",
    "  \"</doc>\")))\n",
    "df = df.drop(fun.col('_2')).drop(fun.col('_1'))\n",
    "\n",
    "df.show(4, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ed827a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.show(1, truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4abe2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('title', fun.split(fun.col('content'), '\\n').getItem(2)) \\\n",
    "        .withColumn('content', fun.split(fun.col('content'), '\\n').getItem(4))\n",
    "df.show(4, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0887b13",
   "metadata": {},
   "source": [
    "### Preparing the Data Using Spark NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510bf4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"content\") \\\n",
    "    .setOutputCol(\"document\") \\\n",
    "    .setCleanupMode(\"shrink\")\n",
    "\n",
    "document_assembler.transform(df).select('document').limit(1).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6946c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split sentence to tokens(array)\n",
    "tokenizer = Tokenizer() \\\n",
    "  .setInputCols([\"document\"]) \\\n",
    "  .setOutputCol(\"token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c8f563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean unwanted characters and garbage\n",
    "normalizer = Normalizer() \\\n",
    "    .setInputCols([\"token\"]) \\\n",
    "    .setOutputCol(\"normalized\") \\\n",
    "    .setLowercase(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25332a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "stopwords_cleaner = StopWordsCleaner()\\\n",
    "      .setInputCols(\"normalized\")\\\n",
    "      .setOutputCol(\"cleanTokens\")\\\n",
    "      .setCaseSensitive(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58163090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stem the words to bring them to the root form.\n",
    "stemmer = Stemmer() \\\n",
    "    .setInputCols([\"cleanTokens\"]) \\\n",
    "    .setOutputCol(\"stem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18d0200",
   "metadata": {},
   "outputs": [],
   "source": [
    "finisher = Finisher() \\\n",
    "    .setInputCols([\"stem\"]) \\\n",
    "    .setOutputCols([\"tokens\"]) \\\n",
    "    .setOutputAsArray(True) \\\n",
    "    .setCleanAnnotations(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c79436e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "nlp_pipeline = Pipeline(\n",
    "    stages=[document_assembler,\n",
    "            tokenizer,\n",
    "            normalizer,\n",
    "            stopwords_cleaner,\n",
    "            stemmer,\n",
    "            finisher])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3872aa40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nlp_model = nlp_pipeline.fit(df)\n",
    "\n",
    "processed_df  = nlp_model.transform(df)\n",
    "\n",
    "processed_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37f2ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_df = processed_df.select('title','tokens')\n",
    "tokens_df.show(2, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c17f1a8",
   "metadata": {},
   "source": [
    "### Computing the TF-IDFa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b4515c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "cv = CountVectorizer(inputCol=\"tokens\", outputCol=\"raw_features\")\n",
    "\n",
    "# train the model\n",
    "cv_model = cv.fit(tokens_df)\n",
    "\n",
    "# transform the data. Output column name will be raw_features.\n",
    "vectorized_tokens = cv_model.transform(tokens_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9ba933",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import IDF\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "\n",
    "idf_model = idf.fit(vectorized_tokens)\n",
    "\n",
    "vectorized_df = idf_model.transform(vectorized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed13632",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_df = vectorized_df.drop(fun.col('raw_features'))\n",
    "\n",
    "vectorized_df.show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2fb555",
   "metadata": {},
   "source": [
    "### Creating Our LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efead7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import LDA\n",
    "\n",
    "num_topics = 5\n",
    "max_iter = 50\n",
    "\n",
    "lda = LDA(k=num_topics, maxIter=max_iter)\n",
    "model = lda.fit(vectorized_df)\n",
    "\n",
    "lp = model.logPerplexity(vectorized_df)\n",
    "\n",
    "print(\"The upper bound on perplexity: \" + str(lp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9122085",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = cv_model.vocabulary\n",
    "\n",
    "raw_topics = model.describeTopics().collect()\n",
    "\n",
    "topic_inds = [ind.termIndices for ind in raw_topics]\n",
    "\n",
    "topics = []\n",
    "for topic in topic_inds:\n",
    "    _topic = []\n",
    "    for ind in topic:\n",
    "        _topic.append(vocab[ind])\n",
    "    topics.append(_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ee0162",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, topic in enumerate(topics, start=1):\n",
    "    print(f\"topic {i}: {topic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a1d98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_df = model.transform(vectorized_df)\n",
    "lda_df.select(fun.col('title'), fun.col('topicDistribution')).\\\n",
    "                show(2, vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3345b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "max_index = fun.udf(lambda x: x.tolist().index(max(x)) + 1, IntegerType())\n",
    "lda_df = lda_df.withColumn('topic_index',\n",
    "                        max_index(fun.col('topicDistribution')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30744591",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_df.select('title', 'topic_index').show(10, truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
